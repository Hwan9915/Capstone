{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "def OrderDataLoader(fname, tokenizer, train_batch_size,valid_batch_size, max_length,split=False, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Build Data Loader\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # json읽어와서 dataset으로 변환\n",
    "    dataset = Dataset.from_json(fname)\n",
    "    \n",
    "    # cls token과 sep token이 설정되어있지 않으면 설정\n",
    "    if not tokenizer.cls_token:\n",
    "        tokenizer.cls_token = tokenizer.eos_token\n",
    "    if not tokenizer.sep_token:\n",
    "        tokenizer.sep_token = tokenizer.eos_token\n",
    "\n",
    "    # TemplateProcessing\n",
    "    tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{tokenizer.cls_token} $0 {tokenizer.sep_token}\",\n",
    "        pair=f\"{tokenizer.cls_token} $A {tokenizer.sep_token} $B:1 {tokenizer.sep_token}:1\",\n",
    "        special_tokens=[(tokenizer.cls_token, tokenizer.cls_token_id), (tokenizer.sep_token, tokenizer.sep_token_id)],\n",
    "    )\n",
    "\n",
    "    # 전처리 함수\n",
    "    def preprocess_function(examples):\n",
    "        processed = {}\n",
    "        inp = f'{examples[\"Text\"]}'\n",
    "       \n",
    "        # 입력에 대한 토큰화\n",
    "        tokenizer_input = tokenizer(\n",
    "            inp,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "        processed[\"input_ids\"] = tokenizer_input[\"input_ids\"]\n",
    "        processed[\"attention_mask\"] = tokenizer_input[\"attention_mask\"]\n",
    "        \n",
    "        # 훈련모드인 경우\n",
    "        if mode == \"train\":\n",
    "            #print(examples[\"code\"])\n",
    "             \n",
    "            # 출력에 대한 토큰화\n",
    "            tokenizer_output = tokenizer(\n",
    "                examples[\"function\"], \n",
    "                padding=\"max_length\", \n",
    "                max_length=max_length, \n",
    "                truncation=True\n",
    "            )\n",
    "            processed[\"decoder_input_ids\"] = tokenizer_output[\"input_ids\"]\n",
    "            processed[\"decoder_attention_mask\"] = tokenizer_output[\"attention_mask\"]\n",
    "        \n",
    "        # 토큰화된 배열 반환\n",
    "        return processed\n",
    "    \n",
    "\n",
    "    # dataset에 대해 전처리 함수로 매핑, columns제거 및 torch tensor로 변환\n",
    "    dataset = dataset.map(preprocess_function,remove_columns=dataset.column_names).with_format(\"torch\")\n",
    "\n",
    "    # dataset을 dataloader로 변환\n",
    "\n",
    "    dataset = dataset.train_test_split(0.2)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=train_batch_size, num_workers=8, pin_memory=True)\n",
    "    valid_dataloader = DataLoader(dataset['test'], shuffle=True, batch_size=valid_batch_size, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "\n",
    "class StoryModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        model: BART model\n",
    "        total_steps: total training steps for lr scheduling\n",
    "        max_learning_rate: Max LR\n",
    "        min_learning_rate: Min LR\n",
    "        warmup_rate: warmup step rate\n",
    "        model_save_dir: path to save model\n",
    "        r3f_lambda: R3F parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        model_save_dir,\n",
    "        total_steps,\n",
    "        max_learning_rate: float = 2e-4,\n",
    "        min_learning_rate: float = 2e-5,\n",
    "        warmup_rate: float = 0.1,\n",
    "        r3f_lambda: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.total_steps = total_steps\n",
    "        self.max_learning_rate = max_learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.warmup_rate = warmup_rate\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.r3f_lambda = r3f_lambda\n",
    "        self.validation_step_loss = []\n",
    "\n",
    "        self.save_hyperparameters(\n",
    "            {\n",
    "                **model.config.to_dict(),\n",
    "                \"total_steps\": total_steps,\n",
    "                \"max_learning_rate\": self.max_learning_rate,\n",
    "                \"min_learning_rate\": self.min_learning_rate,\n",
    "                \"warmup_rate\": self.warmup_rate,\n",
    "                \"r3f_lambda\": self.r3f_lambda,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            decoder_input_ids=batch[\"decoder_input_ids\"],\n",
    "            decoder_attention_mask=batch[\"decoder_attention_mask\"],\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        labels = batch[\"decoder_input_ids\"][:, 1:].reshape(-1)\n",
    "        logits = output[\"logits\"][:, :-1].reshape([labels.shape[0], -1])\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=self.model.config.pad_token_id)\n",
    "        metrics = {\"loss\": loss}\n",
    "        self.log_dict(metrics, prog_bar=True, logger=True, on_step=True)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            decoder_input_ids=batch[\"decoder_input_ids\"],\n",
    "            decoder_attention_mask=batch[\"decoder_attention_mask\"],\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        labels = batch[\"decoder_input_ids\"][:, 1:].reshape(-1)\n",
    "        logits = output[\"logits\"][:, :-1].reshape([labels.shape[0], -1])\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=self.model.config.pad_token_id)\n",
    "        metrics = {\"loss(v)\": loss}\n",
    "        self.validation_step_loss.append(loss)\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, *args, **kwargs):\n",
    "        return self.validation_step(*args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(params=self.model.parameters(), lr=self.max_learning_rate)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.trainer.is_global_zero:\n",
    "            losses = [output.mean() for output in self.validation_step_loss]\n",
    "            loss_mean = sum(losses) / len(losses)\n",
    "\n",
    "            self.model.save_pretrained(\n",
    "                os.path.join(\n",
    "                    self.model_save_dir,\n",
    "                    f\"model-{self.current_epoch:02d}epoch-{self.global_step}steps-{loss_mean:.4f}loss\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.validation_step_loss.clear()  # free memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11cf0b6b7799e034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_logger(name: str) -> logging.Logger:\n",
    "    \"\"\"Return logger for logging\n",
    "\n",
    "    Args:\n",
    "        name: logger name\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.propagate = False\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter(\"[%(asctime)s] %(message)s\"))\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f273a6cee113ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(prog=\"train\", description=\"Train Table to Text with BART\")\n",
    "\n",
    "g = parser.add_argument_group(\"Common Parameter\")\n",
    "g.add_argument(\"--output-dir\", type=str, required=True, help=\"output directory path to save artifacts\")\n",
    "g.add_argument(\"--model-path\", type=str, default=\"gogamza/kobart-base-v2\", help=\"model file path\")\n",
    "g.add_argument(\"--tokenizer\", type=str, default=\"gogamza/kobart-base-v2\", help=\"huggingface tokenizer path\")\n",
    "g.add_argument(\"--gpus\", nargs='+', type=int, required=True, help=\"the number of gpus\")\n",
    "g.add_argument(\"--epochs\", type=int, default=10, help=\"the numnber of training epochs\")\n",
    "g.add_argument(\"--max-learning-rate\", type=float, default=2e-4, help=\"max learning rate\")\n",
    "g.add_argument(\"--min-learning-rate\", type=float, default=1e-5, help=\"min Learning rate\")\n",
    "g.add_argument(\"--warmup-rate\", type=float, default=0.1, help=\"warmup step rate\")\n",
    "g.add_argument(\"--r3f-lambda\", type=float, default=0.1, help=\"r3f lambda\")\n",
    "g.add_argument(\"--max-seq-len\", type=int, default=512, help=\"max sequence length\")\n",
    "g.add_argument(\"--batch-size-train\", type=int, required=True, help=\"training batch size\")\n",
    "g.add_argument(\"--batch-size-valid\", type=int, required=True, help=\"validation batch size\")\n",
    "g.add_argument(\"--logging-interval\", type=int, default=100, help=\"logging interval\")\n",
    "g.add_argument(\"--evaluate-interval\", type=float, default=1.0, help=\"validation interval\")\n",
    "g.add_argument(\"--accumulate-grad-batches\", type=int, default=1, help=\" the number of gradident accumulation steps\")\n",
    "g.add_argument(\"--seed\", type=int, default=42, help=\"random seed\")\n",
    "\n",
    "g = parser.add_argument_group(\"Wandb Options\")\n",
    "g.add_argument(\"--wandb-run-name\", type=str, help=\"wanDB run name\")\n",
    "g.add_argument(\"--wandb-entity\", type=str, help=\"wanDB entity name\")\n",
    "g.add_argument(\"--wandb-project\", type=str, help=\"wanDB project name\")\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    logger = get_logger(\"train\")\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    logger.info(f'[+] Save output to \"{args.output_dir}\"')\n",
    "\n",
    "    logger.info(\" ====== Arguements ======\")\n",
    "    for k, v in vars(args).items():\n",
    "        logger.info(f\"{k:25}: {v}\")\n",
    "\n",
    "    logger.info(f\"[+] Set Random Seed to {args.seed}\")\n",
    "    pl.seed_everything(args.seed)\n",
    "\n",
    "    logger.info(f\"[+] GPU: {args.gpus}\")\n",
    "\n",
    "    logger.info(f'[+] Load Tokenizer\"')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "\n",
    "    logger.info(f'[+] Load Dataset')\n",
    "    train_dataloader, valid_dataloader = OrderDataLoader(\"/home/hgjeong/hdd1/Project/Capstone/train.jsonl\", tokenizer, args.batch_size_train, args.batch_size_valid, args.max_seq_len)\n",
    "\n",
    "    total_steps = len(train_dataloader) * args.epochs // len(args.gpus)\n",
    "\n",
    "    \n",
    "    logger.info(f'[+] Load Model from \"{args.model_path}\"')\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_path)\n",
    "\n",
    "    logger.info(f\"[+] Load Pytorch Lightning Module\")\n",
    "    lightning_module = StoryModule(\n",
    "        model,\n",
    "        args.output_dir,\n",
    "        total_steps,\n",
    "        args.max_learning_rate,\n",
    "        args.min_learning_rate,\n",
    "        args.warmup_rate,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"[+] Start Training\")\n",
    "    train_loggers = [TensorBoardLogger(args.output_dir, \"\", \"logs\")]\n",
    "    if args.wandb_project:\n",
    "        train_loggers.append(\n",
    "            WandbLogger(\n",
    "                name=args.wandb_run_name or os.path.basename(args.output_dir),\n",
    "                project=args.wandb_project,\n",
    "                entity=args.wandb_entity,\n",
    "                save_dir=args.output_dir,\n",
    "           )\n",
    "        )\n",
    "    \n",
    "    # pass a float in the range [0.0, 1.0] to check after a fraction of the training epoch.\n",
    "    # pass an int to check after a fixed number of training batches.\n",
    "    if args.evaluate_interval == 1:\n",
    "        args.evaluate_interval = 1.0\n",
    "    trainer = pl.Trainer(\n",
    "        strategy=\"auto\",\n",
    "        accelerator=\"gpu\",\n",
    "        #logger=train_loggers,\n",
    "        max_epochs=args.epochs,\n",
    "        log_every_n_steps=args.logging_interval,\n",
    "        val_check_interval=args.evaluate_interval,\n",
    "        accumulate_grad_batches=args.accumulate_grad_batches,\n",
    "        callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
    "        devices=args.gpus,\n",
    "    )\n",
    "    trainer.fit(lightning_module, train_dataloader, valid_dataloader)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #args = parser.parse_args(['--dataset', 'SemEval2010', '--plm', 'Llama2', '--mode', 'Both'])\n",
    "\n",
    "    original_args = [\n",
    "    '--output-dir', 'test',\n",
    "    '--model-path', 'chunwoolee0/circulus-kobart-en-to-ko',\n",
    "    '--tokenizer', 'chunwoolee0/circulus-kobart-en-to-ko',\n",
    "    '--gpus', '0',  # If '0 1' is a single argument, replace with '--gpus', '0 1'\n",
    "    '--epoch', '100',\n",
    "    '--max-learning-rate', '2e-5',\n",
    "    '--min-learning-rate', '1e-6',\n",
    "    '--warmup-rate', '0.1',\n",
    "    '--r3f-lambda', '0.1',\n",
    "    '--max-seq-len', '128',\n",
    "    '--batch-size-train', '64',\n",
    "    '--batch-size-valid', '8',\n",
    "    '--logging-interval', '100',\n",
    "    '--evaluate-interval', '1.0',\n",
    "    '--seed', '93',\n",
    "    '--wandb-project', 'capstone'\n",
    "]\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    main(parser.parse_args(original_args))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df93d5abd07c9d3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "708adb3740e70bb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
