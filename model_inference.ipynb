{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import torch\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained('chunwoolee0/circulus-kobart-en-to-ko')\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained('/home/hgjeong/hdd1/Project/Capstone/test/model-31epoch-928steps-0.0001loss')  # 여기서 모델 경로를 직접 지정한 경로 대신 원래의 pko-t5-large로 변경했습니다.\n",
    "#tokenizer.push_to_hub(\"giliit/capstone_tokenizer\")\n",
    "#model.push_to_hub(\"giliit/capstone_v2\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('giliit/capstone_tokenizer')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('giliit/capstone_v2')\n",
    "\n",
    "if not tokenizer.cls_token:\n",
    "        tokenizer.cls_token = tokenizer.eos_token\n",
    "if not tokenizer.sep_token:\n",
    "    tokenizer.sep_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=f\"{tokenizer.cls_token} $0 {tokenizer.sep_token}\",\n",
    "    pair=f\"{tokenizer.cls_token} $A {tokenizer.sep_token} $B:1 {tokenizer.sep_token}:1\",\n",
    "    special_tokens=[(tokenizer.cls_token, tokenizer.cls_token_id), (tokenizer.sep_token, tokenizer.sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate(text):\n",
    "    processed = {}\n",
    "    inp = f'{input_text}'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 입력을 처리하기\n",
    "    tokenizer_input = tokenizer(\n",
    "        inp,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # 모델과 데이터를 GPU로 이동\n",
    "    processed[\"input_ids\"] = tokenizer_input[\"input_ids\"].to(device)\n",
    "    processed[\"attention_mask\"] = tokenizer_input[\"attention_mask\"].to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 모델 실행\n",
    "    summary_tokens = model.generate(\n",
    "            processed[\"input_ids\"],\n",
    "            attention_mask=processed[\"attention_mask\"],\n",
    "            decoder_start_token_id=tokenizer.bos_token_id,\n",
    "            max_length=256,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=7,\n",
    "        )\n",
    "    \n",
    "    # 출력 처리\n",
    "    output = summary_tokens.cpu().detach().tolist()\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb3b8f1d1a77886a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_text = \"아이스 아메리카노랑 뜨아 3개씩 줘\"\n",
    "\n",
    "generate(input_text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8de9c77caeea2b1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
